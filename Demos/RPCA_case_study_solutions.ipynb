{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stephenbeckr/convex-optimization-class/blob/master/Demos/RPCA_case_study_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzewDeFtJ0Sk"
      },
      "source": [
        "# Case study: RPCA\n",
        "\n",
        "In-class challenge: how many different algorithms could you use to solve the RPCA problem?\n",
        "\n",
        "APPM 5630 Adv Convex Optimization, Spring 2023, Professor Stephen Becker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J220FyIlJ0Sm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import linalg\n",
        "from numpy.linalg import norm\n",
        "from numpy.random import default_rng\n",
        "import cvxpy as cvx\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "!wget -nv 'https://github.com/stephenbeckr/convex-optimization-class/raw/master/utilities/firstOrderMethods.py'\n",
        "!wget -nv 'https://github.com/stephenbeckr/convex-optimization-class/raw/master/utilities/secondOrderMethods.py'\n",
        "from firstOrderMethods import gradientDescent, lassoSolver, createTestProblem, runAllTestProblems, DouglasRachford, bookkeeper\n",
        "from secondOrderMethods import NewtonsMethod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7bt9GwyJ0Sn"
      },
      "source": [
        "## The Robust PCA (RPCA) problem\n",
        "$$\\min_{L,S} \\;\\frac12\\|L+S-Y\\|_F^2 + \\lambda_L\\|L\\|_* + \\lambda_S\\|S\\|_1$$\n",
        "where $L,S$ are matrices of the same size; $Y$ is the target matrix that we wish to approximately decompose, $Y \\approx L + S$, into a low-rank part $L$ and a sparse part $S$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J34yB7NTJ0Sn"
      },
      "outputs": [],
      "source": [
        "rng = default_rng(1)\n",
        "m,n = 7,8\n",
        "\n",
        "# for use later\n",
        "def vec(L,S):\n",
        "    return np.concatenate( (L.ravel(), S.ravel() ))\n",
        "def mat( LS ):\n",
        "    L = LS[:m*n].reshape( (m,n ))\n",
        "    S = LS[m*n:].reshape( (m,n ))\n",
        "    return L, S\n",
        "\n",
        "# Generate a \"signal\". Need not be solution to optimization problem\n",
        "rr  = 4\n",
        "LL  = rng.standard_normal( (m,rr) ) @ rng.standard_normal( (rr,n) ) # low-rank\n",
        "SS  = rng.standard_normal( (m,n) )\n",
        "SS[ np.abs(SS) < np.quantile( np.abs(SS), .9 ) ] = 0  # sparse\n",
        "Y   = LL + SS + .01*rng.standard_normal( (m,n) ) # Noisy observations\n",
        "\n",
        "# with np.printoptions(precision=3, suppress=True):\n",
        "#     print(SS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V12veQnEJ0So"
      },
      "source": [
        "### Solve in CVXPY for reference solution\n",
        "\n",
        "This is one way to solve: the $\\ell_1$ and nuclear norms can be written in terms of epigraphs of standard cones and such, and then solved via standard methods (IPM, ADMM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKP2TClxJ0So",
        "outputId": "1dfe40c1-257e-4076-80e3-86ec0d99fa52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[10.374  4.849  2.435  1.788  0.466  0.314  0.   ]\n",
            "[10.373  4.687  2.211  1.507  0.     0.     0.   ]\n",
            "[[ 0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
            " [-1.649  0.     0.     0.     0.     0.     0.     0.   ]\n",
            " [ 0.    -1.482  0.     0.     0.     0.    -1.631  0.   ]\n",
            " [ 0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
            " [ 0.     0.     0.    -2.251  0.     0.     0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.     0.     0.    -1.514]\n",
            " [ 1.753  0.     0.     0.     0.     0.     0.     0.   ]]\n",
            "[[-0.    -0.    -0.     0.    -0.     0.     0.    -0.   ]\n",
            " [-0.54   0.     0.     0.     0.     0.    -0.     0.   ]\n",
            " [ 0.    -0.61  -0.     0.    -0.     0.    -0.666 -0.   ]\n",
            " [-0.    -0.    -0.    -0.     0.    -0.671 -0.    -0.   ]\n",
            " [ 0.     0.    -0.    -1.694 -0.     0.    -0.     0.   ]\n",
            " [ 0.     0.    -0.    -0.     0.     0.    -0.    -0.912]\n",
            " [ 0.     0.    -0.188  0.     0.    -0.    -0.     0.   ]]\n"
          ]
        }
      ],
      "source": [
        "# Solve in cvxpy\n",
        "L     = cvx.Variable( (m,n) )\n",
        "S     = cvx.Variable( (m,n) )\n",
        "lambdaL = 3e-3\n",
        "lambdaS = 1.7e-3\n",
        "\n",
        "def prox_l1( X, t, lambdaS ):\n",
        "    return np.sign(X)*np.maximum( 0, np.fabs(X) - lambdaS*t )\n",
        "\n",
        "def prox_nuclearNorm( X, t, lambdaL ):\n",
        "   U, s, Vh = linalg.svd(X, full_matrices=False )\n",
        "   s = prox_l1( s, t, lambdaL )\n",
        "   return U@( s.reshape((-1,1))*Vh )\n",
        "\n",
        "def objective_vec( LSvec ):\n",
        "    L,S = mat(LSvec)\n",
        "    resid = norm( L+S - Y)\n",
        "    return resid**2/2 + lambdaL*np.sum( linalg.svdvals(L) ) + lambdaS*norm( S.ravel(), ord=1)\n",
        "obj   = cvx.Minimize( cvx.sum_squares(L+S-Y)/2 + lambdaL*cvx.norm(L, \"nuc\") + lambdaS*cvx.pnorm(S,p=1) )\n",
        "prob  = cvx.Problem(obj)\n",
        "highPrecision = {'solver':cvx.CVXOPT,'max_iters':400,'abstol':1e-11,'reltol':1e-11}\n",
        "prob.solve(**highPrecision)\n",
        "L_CVX = L.value \n",
        "S_CVX = S.value \n",
        "fTrue = prob.value\n",
        "\n",
        "with np.printoptions(precision=3, suppress=True):\n",
        "    print( linalg.svdvals( L_CVX ))\n",
        "    print( linalg.svdvals( LL ) )\n",
        "    print(SS)\n",
        "    print(S_CVX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78kJ1UhyJ0Sp"
      },
      "source": [
        "# The algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3KIhcNkJ0Sp"
      },
      "source": [
        "### Proximal gradient descent (or FISTA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMtYs0SwJ0Sp",
        "outputId": "97024230-35ad-44d1-ac9c-82c44fed346f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.06968472982246328\n",
            "Iter.  Objective Stepsize  Error\n",
            "-----  --------- --------  -------\n",
            "    0  9.38e-02  5.00e-01  6.79e-01\n",
            "  500  6.97e-02  5.00e-01  2.04e-02\n",
            " 1000  6.97e-02  5.00e-01  1.96e-03\n",
            " 1500  6.97e-02  5.00e-01  2.45e-04\n",
            " 2000  6.97e-02  5.00e-01  3.11e-05\n",
            " 2500  6.97e-02  5.00e-01  3.94e-06\n",
            " 2873  6.97e-02  5.00e-01  3.35e-07\n",
            "Iter 2873 Quitting due to stagnating objective value\n"
          ]
        }
      ],
      "source": [
        "def prox( LSvec, t ):\n",
        "    L,S = mat(LSvec)\n",
        "    L   = prox_nuclearNorm( L, t, lambdaL )\n",
        "    S   = prox_l1( S, t, lambdaS)\n",
        "    return vec( L, S )\n",
        "def prox_obj( LSvec ):\n",
        "    L,S = mat(LSvec)\n",
        "    return lambdaL*np.sum( linalg.svdvals(L) ) + lambdaS*norm( S.ravel(), ord=1)\n",
        "\n",
        "def f( LSvec ):\n",
        "    L,S = mat(LSvec)\n",
        "    resid = norm( L+S - Y)\n",
        "    return resid**2/2\n",
        "\n",
        "def grad( LSvec ):\n",
        "    L,S = mat(LSvec)\n",
        "    res = L+S-Y\n",
        "    return np.concatenate( ( res.ravel(), res.ravel() ) )\n",
        "    \n",
        "print( fTrue )\n",
        "refSoln = vec( L_CVX, S_CVX)\n",
        "errFcn  = lambda LS : norm( LS-refSoln )/norm(refSoln)\n",
        "LSvec0 = np.zeros(2*m*n)  # starting point\n",
        "LS, data = gradientDescent( f, grad, LSvec0, prox, prox_obj, stepsize = 0.5, tol=1e-16, maxIters=1e4,\n",
        "    errorFunction=errFcn, ArmijoLinesearch=False, acceleration=True, restart=500, saveHistory=True )\n",
        "L,S = mat(LS)\n",
        "# with np.printoptions(precision=3, suppress=True):\n",
        "#     print( L )\n",
        "#     print( S )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-BXEA2rJ0Sq"
      },
      "source": [
        "### Variable Projection  (VarPro) + proximal gradient descent\n",
        "$$\\min_{L} \\lambda_L\\|L\\|_* + \\underbrace{\\min_S \\frac12\\|L+S-Y\\|_F^2+\\lambda_S\\|S\\|_1}_{f(L)}$$\n",
        "where\n",
        "$$\\nabla f(L) = L+S-Y\\quad\\text{where }S\\text{ solves the partial minimization problem}\n",
        "$$\n",
        "\n",
        "and of course you could also switch this around and flip the roles of $S$ and $L$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY4KaaktJ0Sq",
        "outputId": "7d323843-b5ec-4b72-f6ec-3cd52f2f03f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.06968472982246328\n",
            "Iter.  Objective Stepsize  Error\n",
            "-----  --------- --------  -------\n",
            "    0  1.15e-01  5.00e-01  1.39e+00\n",
            "  500  6.98e-02  5.00e-01  5.58e-02\n",
            " 1000  6.97e-02  5.00e-01  7.36e-04\n",
            " 1500  6.97e-02  5.00e-01  8.33e-06\n",
            " 1757  6.97e-02  5.00e-01  6.29e-07\n",
            "Iter 1757 Quitting due to stagnating objective value\n"
          ]
        }
      ],
      "source": [
        "def S_from_L( L ):\n",
        "    return prox_l1( Y-L, 1, lambdaS )\n",
        "\n",
        "def prox( Lvec, t ):\n",
        "    L   = Lvec.reshape( (m,n) )\n",
        "    L   = prox_nuclearNorm( L, t, lambdaL )\n",
        "    return L.ravel()\n",
        "    \n",
        "def prox_obj( Lvec ):\n",
        "    L   = Lvec.reshape( (m,n) )\n",
        "    return lambdaL*np.sum( linalg.svdvals(L) )\n",
        "\n",
        "def f( Lvec ):\n",
        "    L   = Lvec.reshape( (m,n) )\n",
        "    S   = S_from_L(L)\n",
        "    resid = norm( L+S - Y)\n",
        "    return resid**2/2 + lambdaS*norm(S.ravel(),ord=1)\n",
        "\n",
        "def grad( Lvec ):\n",
        "    L   = Lvec.reshape( (m,n) )\n",
        "    S   = S_from_L(L)\n",
        "    res = L+S-Y\n",
        "    return res.ravel()\n",
        "    \n",
        "print( fTrue )\n",
        "refSoln = vec( L_CVX, S_CVX)\n",
        "def errFcnL( Lvec ):\n",
        "    L   = Lvec.reshape( (m,n) )\n",
        "    S   = S_from_L(L)\n",
        "    return norm( vec(L,S)-refSoln )/norm(refSoln)\n",
        "\n",
        "LSvec0 = np.zeros(2*m*n)  # starting point\n",
        "Lvec, data = gradientDescent( f, grad, np.zeros(m*n), prox, prox_obj, stepsize = 0.5, tol=1e-16, maxIters=1e4,\n",
        "    errorFunction=errFcnL, ArmijoLinesearch=False, acceleration=True, restart=250, saveHistory=True )\n",
        "    # TODO: if restart < 0, set saveHistory = True\n",
        "    # NOTE: I fiddled with fNew, make sure that still passes unit tests. Should do better?\n",
        "L   = Lvec.reshape( (m,n) )\n",
        "S   = S_from_L(L)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI6D_38pJ0Sq"
      },
      "source": [
        "### Alternating minimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJeUdTp3J0Sq",
        "outputId": "2e7daafe-fc6e-48bb-9f53-1ae366d77357"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter.  Error\n",
            "-----  -------\n",
            "    0  2.69e-01\n",
            " 1500  1.08e-01\n",
            " 3000  4.76e-02\n",
            " 4500  2.62e-02\n",
            " 6000  1.45e-02\n",
            " 7500  7.96e-03\n",
            " 9000  4.33e-03\n",
            "10500  2.35e-03\n",
            "12000  1.27e-03\n",
            "13500  6.86e-04\n",
            "15000  3.70e-04\n",
            "16500  2.00e-04\n",
            "18000  1.08e-04\n",
            "19500  5.81e-05\n",
            "21000  3.13e-05\n",
            "22500  1.69e-05\n",
            "24000  9.07e-06\n",
            "25500  4.84e-06\n",
            "27000  2.56e-06\n",
            "28500  1.33e-06\n"
          ]
        }
      ],
      "source": [
        "L   = Y\n",
        "maxIters = int(3e4)\n",
        "printEvery = int( maxIters/20 )\n",
        "print(\"Iter.  Error\")\n",
        "print(\"-----  -------\")\n",
        "for k in range(maxIters):\n",
        "    S   = prox_l1( Y-L, 1, lambdaS )\n",
        "    L   = prox_nuclearNorm( Y-S, 1, lambdaL )\n",
        "    err = errFcn( vec(L,S) )\n",
        "    if not k % printEvery :\n",
        "        print(f\"{k:5d}  {err:.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ojzK-icJ0Sq"
      },
      "source": [
        "### Douglas-Rachford"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsB9BdCWJ0Sr",
        "outputId": "d8778957-750d-4378-8557-7ccd595a7be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter.  Objective Error\n",
            "-----  --------- -------\n",
            "    0  1.05e-01  6.79e-01\n",
            "   50  7.20e-02  2.88e-01\n",
            "  100  7.01e-02  1.51e-01\n",
            "  150  6.98e-02  8.47e-02\n",
            "  200  6.97e-02  4.31e-02\n",
            "  250  6.97e-02  2.06e-02\n",
            "  300  6.97e-02  9.88e-03\n",
            "  350  6.97e-02  4.73e-03\n",
            "  400  6.97e-02  2.26e-03\n",
            "  450  6.97e-02  1.09e-03\n",
            "  500  6.97e-02  5.22e-04\n",
            "  550  6.97e-02  2.51e-04\n",
            "  600  6.97e-02  1.21e-04\n",
            "  650  6.97e-02  5.82e-05\n",
            "  700  6.97e-02  2.80e-05\n",
            "  750  6.97e-02  1.35e-05\n",
            "  800  6.97e-02  6.41e-06\n",
            "  850  6.97e-02  3.00e-06\n",
            "  900  6.97e-02  1.36e-06\n",
            "  936  6.97e-02  7.39e-07\n",
            "==  Quitting due to stagnating objective value  ==\n"
          ]
        }
      ],
      "source": [
        "def prox1( LSvec, t ):\n",
        "    L,S = mat(LSvec)\n",
        "    L   = prox_nuclearNorm( L, t, lambdaL )\n",
        "    S   = prox_l1( S, t, lambdaS)\n",
        "    return vec( L, S )\n",
        "\n",
        "def prox2( LSvec, t ):\n",
        "    L,S = mat(LSvec)\n",
        "    L_new   = -t*(S - (1+t)/t*L - Y)/(1+2*t)\n",
        "    S_new   = -t*(L - (1+t)/t*S - Y)/(1+2*t)\n",
        "    return vec( L_new, S_new )\n",
        "\n",
        "LSvec0 = np.zeros(2*m*n)\n",
        "LS, data = DouglasRachford( prox1, prox2, LSvec0, gamma=4e1, F=objective_vec, overrelax = 1.999, \n",
        "    tol =1e-10, maxIters = 1e3, errorFunction=errFcn, printEvery=None )\n",
        "\n",
        "L,S = mat(LS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVdAW0-aJ0Sr"
      },
      "source": [
        "### Primal-dual method\n",
        "(Chambolle-Pick / Vu / Condat)\n",
        "\n",
        "TBD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrPiPxQUJ0Sr"
      },
      "source": [
        "### Burer-Monteiro splitting on $\\|L\\|_*$ term\n",
        "\n",
        "TBD\n",
        "\n",
        "Exploit the fact that $\\|L\\|_* = \\min_{U,V}\\; \\frac12(\\|U\\|_F^2 + \\|V\\|_F^2) \\quad\\text{s.t.}UV^T = L$ and make a change of variables\n",
        "\n",
        "cf. [Adapting Regularized Low Rank Models for Parallel Architectures](https://doi.org/10.1137/17M1147342) Derek Driggs, Stephen Becker, Aleksandr Aravkin (SISC, 2019)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}