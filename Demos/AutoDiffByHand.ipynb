{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHDKTlgKg1VowNIgcB9vFA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stephenbeckr/convex-optimization-class/blob/main/Demos/AutoDiffByHand.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forward vs Reverse mode Autodiff: explicit example\n",
        "Copied from 2024 SciML class\n",
        "#### Background\n",
        "If $F: \\mathbb{R}^n \\to \\mathbb{R}^m$, we can write $F(\\vec{x}) = ( F_i(\\vec{x}) )_{i=1}^m$ for component functions $F_i : \\mathbb{R}^n \\to \\mathbb{R}$, and we define the Jacobian $J_F$ to be the $m\\times n$ matrix of partial derivatives, so that the $(i,j)$ entry of $J_F$ is $\\frac{\\partial F_i}{\\partial x_j}(\\vec{x})$.  *Note that if $m=1$ then the Jacobian is just the transpose of the gradient.*\n",
        "\n",
        "For multivariate functions, because derivatives (i.e., the Jacobian) are matrices and because matrix multiplication does not commute, we have to be careful with the order we write the chain rule in. The correct order is:\n",
        "$$J_{f \\circ g}(\\vec{x}) = J_f(g(\\vec{x})) \\cdot J_g(\\vec{x}).$$"
      ],
      "metadata": {
        "id": "Rzg8FgvJlBvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "print(\"Torch version is\", torch.__version__)\n",
        "print(\"Numpy version is\", np.__version__)\n",
        "print(\"Python version is\", sys.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNL8XpH9lIDo",
        "outputId": "55ad982f-de30-4b9a-b840-7c4885387851"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version is 2.6.0+cu124\n",
            "Numpy version is 2.0.2\n",
            "Python version is 3.11.12 (main, Apr  9 2025, 08:55:54) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 1\n",
        "We'll explore forward-mode and reverse-mode automatic differentiation. Implementing autodiff to work in general requires a lot of programming (especially for reverse-mode), so instead we'll specialize to one particular function, and choose a function with a straightforward \"computational graph\". Let\n",
        "$$f:\\mathbb{R}^{d_0} \\to \\mathbb{R}, \\quad f(\\vec{x}) = \\text{sum}\\left( \\sigma( B \\cdot \\sigma( A \\cdot \\vec{x} ) ) \\right)$$\n",
        "where $\\vec{x}\\in\\mathbb{R}^{d_0}$, $A \\in \\mathbb{R}^{d_1 \\times d_0}$, $B\\in \\mathbb{R}^{d_2\\times d_1}$ and $\\sigma(\\alpha) = (1+e^{-\\alpha})^{-1}$ is the 1D logistic function (aka *the* \"sigmoid\" in ML terminology) and $\\sigma$ applied to a vector is done componentwise.  Basically, this is a simple feed-forward neural net.  We're going to compute the gradient of $f$, $\\nabla f(\\vec{x})$, aka $J_f(\\vec{x})^\\top$.  *Be careful, in neural net training, we take gradients with respect to the weight matrices, but in this problem we are thinking of the weights as fixed and differentiating with respect to $\\vec{x}$, since that's slightly simpler since it's a vector not a matrix*\n",
        "\n",
        "#### Part 1a\n",
        "Let $h_1(\\vec{x}) = A \\cdot \\vec{x}$, $h_2( \\vec{y} ) = \\sigma(\\vec{y})$, $h_3(\\vec{y}) = B\\cdot \\vec{y}$, $h_4= h_2$, and $h_5(\\vec{y})= \\text{sum}(\\vec{y})$.\n",
        "Then\n",
        "$$f(\\vec{x}) = \\text{sum}\\Big( \\overbrace{\\sigma( \\overbrace{B \\cdot \\underbrace{\\sigma( \\underbrace{A \\cdot \\vec{x}}_{\\vec{y}_1} )}_{\\vec{y}_2} }^{\\vec{y}_3} )}^{\\vec{y}_4} \\Big)\n",
        "= h_5(h_4(h_3(h_2(h_1(\\vec{x})))))$$\n",
        "so we can write the Jacobian of $F$ as\n",
        "$$J_f(\\vec{x}) = J_{h_5}( \\vec{y}_4) \\cdot J_{h_4}( \\vec{y}_3 ) \\cdot\n",
        " J_{h_3}( \\vec{y}_2 ) \\cdot\n",
        "  J_{h_2}( \\vec{y}_1 ) \\cdot J_{h_1}( \\vec{x} ).$$\n",
        "For part (a), mathematically work out what the Jacobian of each of the $h_k$ functions is and write out your answer."
      ],
      "metadata": {
        "id": "NPqHdlE8lNLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1b: implement the function\n",
        "Implement the function $f$ in code, and use an existing automatic differentiation package (I suggest PyTorch) to get the gradient, which we will later use to check the correctness of our code.  *I suggest choosing moderate values for $d_0,d_1,d_2$ and make these values different to help find bugs in your code.  The matrices $A$ and $B$ can be arbitrary, e.g., random.*"
      ],
      "metadata": {
        "id": "_CrkbAENlUjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sigma = torch.nn.Sigmoid() # really \"logistic function\"\n",
        "\n",
        "d0 = 100\n",
        "d1 = 105\n",
        "d2 = 95\n",
        "torch.manual_seed(100)\n",
        "# dtype = torch.float32 # the default\n",
        "dtype = torch.float64\n",
        "A   = torch.randn( (d1,d0), dtype=dtype) # parameters\n",
        "B   = torch.randn( (d2,d1), dtype=dtype )\n",
        "x   = torch.randn( (d0,1), dtype=dtype , requires_grad=True )\n",
        "\n",
        "def f(x):\n",
        "    return torch.sum( sigma( B@sigma(A@x)) )\n",
        "\n",
        "\n",
        "f(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3emgaYGrlLOK",
        "outputId": "e4e447bd-8a40-4e12-d6b7-81a0a7fb6e0b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(53.0627, dtype=torch.float64, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1c: implement the gradient, forward-style\n",
        " Implement a gradient for your function in the forward-mode style. That is, calculate\n",
        "$$J_f(\\vec{x}) = J_{h_5}( \\vec{y}_4) \\cdot \\Bigg( J_{h_4}( \\vec{y}_3 ) \\cdot\n",
        " \\Big(J_{h_3}( \\vec{y}_2 ) \\cdot\n",
        "  \\Big(J_{h_2}( \\vec{y}_1 ) \\cdot J_{h_1}( \\vec{x} )\\Big)\\Big)\\Bigg).$$\n",
        "This gradient function should be in the same function that also calculates $f(\\vec{x})$, so now have that function return two values, $f(\\vec{x})$ and $J_f(\\vec{x})$.\n",
        "Comparing with the autodiff software (PyTorch) at one or more points $\\vec{x}$, make sure you get the right answer. *Note: autodiff software like PyTorch works in single precision by default, so you shouldn't expect your answer to be more than about 5 to 8 digits the same. Try using double precision, and so you should have 10 to 15 digits the same.*\n"
      ],
      "metadata": {
        "id": "tcxBQgfPlY8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dsigmoid(x):\n",
        "    \"\"\"\n",
        "    d/dx sigma(x) for an activation function sigma\n",
        "    Here, we're assuming the activation function is logistic/sigmoid\n",
        "    i.e., sigma(x) = 1/(1+e^{-x}) = e^x / (1+e^x)\n",
        "    so\n",
        "    d/dx sigma(x) = sigma(x)*(1-sigma(x)) = 1/(e^x + 2 + e^{-x})\n",
        "                  = e^x / (1+e^x)^2 = 1/(2*(cosh(x)+2))\n",
        "    \"\"\"\n",
        "    # s = sigma(x)\n",
        "    # return s * (1-s)\n",
        "\n",
        "    return 1/(2*(1+torch.cosh(x)))\n",
        "\n",
        "    # This is *less* stable\n",
        "    # ex =  torch.exp(x)\n",
        "    # return ex / (1+ex)**2\n",
        "\n",
        "def f_and_Jacobian(x, mode='reverse'):\n",
        "    y1 = torch.matmul(A,x)\n",
        "    y2 = torch.matmul(B,sigma(y1) )\n",
        "    fx = torch.sum(sigma(y2))\n",
        "\n",
        "    if mode.lower() == 'reverse':\n",
        "        # compute Jacobian starting at the end (reverse-mode)\n",
        "        # Let's force y2 to be of size (d2,1) rather than (d2,)\n",
        "        #   (right now, it depends on whether input is size (d0,1) or (d,) )\n",
        "        z3 = dsigmoid(y2.reshape((-1,1))) # implicitly doing ones vector times diagonal matrix\n",
        "        z2 = z3.T @ B # z2 is now a row vector\n",
        "        z1 = z2 * dsigmoid(y1.ravel()) # diagonal matrix multiply\n",
        "        J_f = z1 @ A\n",
        "    elif mode.lower() == 'forward':\n",
        "        z2 = dsigmoid(y1.reshape(-1,1) ) * A\n",
        "        z3 = B @ z2\n",
        "        z4 = dsigmoid(y2) * z3\n",
        "        J_f = torch.sum( z4, 0, keepdim=True)\n",
        "    else:\n",
        "        raise ValueError('That mode is not implemented')\n",
        "\n",
        "    return fx, J_f"
      ],
      "metadata": {
        "id": "IwUvggInlZWD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fx, gx = f_and_Jacobian(x.detach(), mode='forward' )\n",
        "gx = gx.T # Gradient is the transpose of the Jacobian\n",
        "\n",
        "# And compute the gradient using PyTorch's autodiff to check our answer\n",
        "if x.grad is not None:\n",
        "    x.grad.data.zero_()\n",
        "out = f(x)\n",
        "out.backward()\n",
        "grad = x.grad\n",
        "\n",
        "# Print out some metrics:\n",
        "print(f'\"allclose\" is {torch.allclose( gx, grad )}')\n",
        "print(f'||gx-grad||_infty|| is {torch.linalg.norm(gx-grad,ord=np.inf):.2e}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrvtxIuElcV-",
        "outputId": "aa3c7d03-8bbe-41c9-b9f8-e2469cdb0fdd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"allclose\" is True\n",
            "||gx-grad||_infty|| is 1.89e-15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1d: implement the gradient, backward-style\n",
        " Implement a gradient for your function in the reverse-mode style. That is, calculate\n",
        "$$\n",
        "J_f(\\vec{x}) = \\Bigg(\\Big(\\Big( J_{h_5}( \\vec{y}_4) \\cdot  J_{h_4}( \\vec{y}_3 ) \\Big) \\cdot\n",
        " J_{h_3}( \\vec{y}_2 ) \\Bigg) \\cdot\n",
        "  J_{h_2}( \\vec{y}_1 ) \\Bigg) \\cdot J_{h_1}( \\vec{x} ).\n",
        "$$\n",
        "Again, implement this in the same function that calculates $f(\\vec{x})$, since you will want to save some intermediate values on the forward pass.\n",
        "Comparing with the autodiff software, make sure you get the right answer."
      ],
      "metadata": {
        "id": "y57gz4qDlhnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Already implemented above\n",
        "\n",
        "fx, gx = f_and_Jacobian(x.detach(), mode='reverse' )\n",
        "gx = gx.T # Gradient is the transpose of the Jacobian\n",
        "\n",
        "# And compute the gradient using PyTorch's autodiff to check our answer\n",
        "if x.grad is not None:\n",
        "    x.grad.data.zero_()\n",
        "out = f(x)\n",
        "out.backward()\n",
        "grad = x.grad\n",
        "\n",
        "# Print out some metrics:\n",
        "print(f'\"allclose\" is {torch.allclose( gx, grad )}')\n",
        "print(f'||gx-grad||_infty|| is {torch.linalg.norm(gx-grad,ord=np.inf):.2e}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCYJ6lyQlda_",
        "outputId": "5be2ad48-3dd7-4966-a21e-956961090cad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"allclose\" is True\n",
            "||gx-grad||_infty|| is 1.67e-15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1e: complexity\n",
        "What is the computational complexity for the forward-mode style (in terms of $d_0,d_1,d_2$)? What about for reverse-mode style?\n",
        "\n",
        "**Solution**\n",
        "\n",
        "- For **forward-mode**, the complexity is $\\mathcal{O}(d_0 d_1 d_2)$\n",
        "- For **reverse-mode**, the complexity is $\\mathcal{O}(d_0 (d_1 + d_2))$, which is much better"
      ],
      "metadata": {
        "id": "yjkF9_p1lnQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1f: run it for a large size\n",
        " Set $d_0 = d_1 = d_2 = 8000$ and choose $A,B$ to be random matrices (double-precision). Time how long it takes your code to run in forward-mode, and how long it takes to run in reverse-mode, and also compare with how long it takes the autodiff package to run."
      ],
      "metadata": {
        "id": "LSsj6cNTlpwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d0 = int(8e3)\n",
        "d1 = d0\n",
        "d2 = d0\n",
        "\n",
        "torch.manual_seed(100)\n",
        "# dtype = torch.float32 # the default\n",
        "dtype = torch.float64\n",
        "A   = torch.randn( (d1,d0), dtype=dtype) # parameters\n",
        "B   = torch.randn( (d2,d1), dtype=dtype )\n",
        "x   = torch.randn( (d0,1), dtype=dtype , requires_grad=True )\n",
        "\n",
        "# We were lazy and \"burned in\" the values of A and B into the function definition,\n",
        "#   so with new \"A\" and \"B\" we need to redefine them:\n",
        "def f(x):\n",
        "    return torch.sum( sigma( B@sigma(A@x)) )\n",
        "def f_and_Jacobian(x, mode='reverse'):\n",
        "    y1 = torch.matmul(A,x)          # we need to store this for use in reverse mode\n",
        "    y2 = torch.matmul(B,sigma(y1) ) # we need to store this for use in reverse mode\n",
        "    fx = torch.sum(sigma(y2))\n",
        "\n",
        "    if mode.lower() == 'reverse':\n",
        "        # compute Jacobian starting at the end (reverse-mode)\n",
        "        # Let's force y2 to be of size (d2,1) rather than (d2,)\n",
        "        #   (right now, it depends on whether input is size (d0,1) or (d,) )\n",
        "        z3 = dsigmoid(y2.reshape((-1,1))) # implicitly doing ones vector times diagonal matrix\n",
        "        z2 = z3.T @ B # z2 is now a row vector\n",
        "        z1 = z2 * dsigmoid(y1.ravel()) # diagonal matrix multiply\n",
        "        J_f = z1 @ A\n",
        "    elif mode.lower() == 'forward':\n",
        "        z2 = dsigmoid(y1.reshape(-1,1) ) * A\n",
        "        z3 = B @ z2\n",
        "        z4 = dsigmoid(y2) * z3\n",
        "        J_f = torch.sum( z4, 0, keepdim=True)\n",
        "    else:\n",
        "        raise ValueError('That mode is not implemented')\n",
        "\n",
        "    return fx, J_f"
      ],
      "metadata": {
        "id": "oeqwfVMxlkRA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "if x.grad is not None:\n",
        "    x.grad.data.zero_()\n",
        "\n",
        "out = f(x)\n",
        "out.backward()\n",
        "grad = x.grad\n",
        "print('== The time for PyTorch to do AutoDiff (it uses reverse mode) ==')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06LgNR38lrmv",
        "outputId": "104f903b-5c99-46de-8b0a-fbbebd6e170e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== The time for PyTorch to do AutoDiff (it uses reverse mode) ==\n",
            "CPU times: user 177 ms, sys: 0 ns, total: 177 ms\n",
            "Wall time: 178 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "fx, gx = f_and_Jacobian(x.detach(), mode='forward' )\n",
        "gx = gx.T\n",
        "print('== The time for our own forward mode ==')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxH859jLltG_",
        "outputId": "be625dbb-b860-425f-a977-6120ad2708c8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== The time for our own forward mode ==\n",
            "CPU times: user 30.7 s, sys: 1.19 s, total: 31.9 s\n",
            "Wall time: 32.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "fx, gx = f_and_Jacobian(x.detach(), mode='reverse' )\n",
        "gx = gx.T\n",
        "print('== The time for our own reverse mode ==')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbiEmwOWluUY",
        "outputId": "fb316eba-fb20-497d-c16e-cdfa6017c394"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== The time for our own reverse mode ==\n",
            "CPU times: user 190 ms, sys: 65 µs, total: 190 ms\n",
            "Wall time: 190 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion: for this kind of function, reverse-mode is much much faster than forward-mode"
      ],
      "metadata": {
        "id": "ZEuTKAXHlyO5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OP14tv-QlyuF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}