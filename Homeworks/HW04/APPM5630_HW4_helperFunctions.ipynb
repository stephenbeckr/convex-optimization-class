{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "APPM5630_HW4_helperFunctions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOA16MOLaHb7sbmbicBlUVh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stephenbeckr/convex-optimization-class/blob/master/Homeworks/HW04/APPM5630_HW4_helperFunctions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxtxY_yDpO8S"
      },
      "source": [
        "# Homework 4\n",
        "University of Colorado Boulder, Spring 2021\n",
        "\n",
        "[APPM 5630 Advanced Convex Optimization](https://github.com/stephenbeckr/convex-optimization-class), by Stephen Becker\n",
        "\n",
        "1D blurring problem\n",
        "\n",
        "This notebook contains **helper functions** (**utilities**) that may be useful in future homeworks\n",
        "1. `implicit2explict`\n",
        "2. Adjoint check\n",
        "3. Power method to find spectral norms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho_qJTgqpVXZ"
      },
      "source": [
        "First, setup some packages, and make a helper routine for printing matrices, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9E4lij1yck2"
      },
      "source": [
        "import numpy as np\n",
        "from numpy.fft import fft, ifft, rfft, irfft\n",
        "from numpy.linalg import norm\n",
        "import scipy.ndimage\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams[\"figure.figsize\"] = [8,6] # or 7, 4 or 10,8\n",
        "mpl.rcParams[\"lines.linewidth\"] = 2\n",
        "mpl.rcParams[\"lines.markersize\"] = 4\n",
        "mpl.rcParams['mathtext.fontset'] = 'cm'\n",
        "mpl.rcParams.update({'font.size': 20})\n",
        "\n",
        "# Helper routine to print out matrices nicely\n",
        "def matprint(mat, fmt=\"g\",roundToDecimal=2):\n",
        "  # from https://gist.github.com/braingineer/d801735dac07ff3ac4d746e1f218ab75\n",
        "  # Modified to round\n",
        "  if roundToDecimal is not None:\n",
        "    mat = np.round(mat,decimals=roundToDecimal)\n",
        "  #col_maxes = [max([len((\"{:\"+fmt+\"}\").format(x)) for x in col]) for col in mat.T]\n",
        "  if np.min( mat.flat ) < 0:\n",
        "    col_maxes = [8 for col in mat.T] # quick hack to deal with fmt='.1e'\n",
        "  else:\n",
        "    col_maxes = [7 for col in mat.T] # quick hack to deal with fmt='.1e'\n",
        "  for x in mat:\n",
        "    for i, y in enumerate(x):\n",
        "      if abs(y) >= 1000:\n",
        "        fmt = '.1e'\n",
        "      else:\n",
        "        fmt = 'g'\n",
        "      print((\"{:\"+str(col_maxes[i])+fmt+\"}\").format(y), end=\"  \")\n",
        "    print(\"\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcaC6feVbJ2v"
      },
      "source": [
        "## `implicit2explicit`\n",
        "We'll get a little bit fancy and allow functions that have matrix inputs (theoretically, this is no big deal, since matrices in $\\mathbb{R}^{m\\times n}$ are isomorphic to vectors in $\\mathbb{R}^{mn}$ when we use the inner product $\\langle X, Y\\rangle = \\text{trace}(X^*Y)$ where $X^*=\\overline{X}^T$ is the adjoint.  This was not necessary for the homework problem.\n",
        "\n",
        "Note that you can specify `domainSize` which could be something like `n` for a 1D vector of length $n$, or sometimes `(n,1)` depending on how the function expects it. For matrices, set `domainSize` to `(m,n)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo6JbiaCph8Z"
      },
      "source": [
        "def implicit2explicit( linearFcn, domainSize ):\n",
        "    \"\"\" domainSize can be an integer, for a pure vector,\n",
        "    or can be a tuple of integers \"\"\"\n",
        "    x = np.zeros( domainSize )\n",
        "    # use ravel not flatten, as we don't want to make a copy\n",
        "\n",
        "    # Determine sizes\n",
        "    n = x.size\n",
        "    x.ravel()[0] = 1  # first unit vector\n",
        "    y = np.asarray( linearFcn( x ) ) # output may be scalar or list (not array) so convert it\n",
        "    m = y.size\n",
        "    x.ravel()[0] = 0 # undo it\n",
        "\n",
        "    # Allocate array\n",
        "    A = np.zeros( (m,n), dtype=y.dtype ) # allow for complex values\n",
        "    A[:,0] = y.ravel()\n",
        "\n",
        "    # Loop over all inputs (we already did the first one)\n",
        "    for col in range(1,n):\n",
        "      x.ravel()[col] = 1  # first unit vector\n",
        "      y = linearFcn( x )\n",
        "      A[:,col] = np.asarray(y).ravel()\n",
        "      x.ravel()[col] = 0 # undo it\n",
        "\n",
        "    return A"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKEPUJ7DGGAj"
      },
      "source": [
        "Be careful! A few students made what seemed like a valid implementation of `implicit2explicit` (specialized to the square operator case) using the identity matrix, but then ran into trouble due to Python's row-based indexing. See the issue below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPt6aD3wGdcU",
        "outputId": "d83b18aa-362d-4337-d342-aeeb4401a14d"
      },
      "source": [
        "def implicit2explicit_square_buggy( linearFcn, n ):\n",
        "  I   = np.eye(n)\n",
        "  y   = linearFcn( I[0] )\n",
        "  A   = np.zeros( (n,n), dtype=y.dtype )\n",
        "  for col in range(0,n):\n",
        "    A[col] = linearFcn( I[col] )\n",
        "  return A\n",
        "\n",
        "# linearFcn = fft\n",
        "A   = np.arange(16).reshape(4,4) # 4 x 4 matrix\n",
        "linearFcn = lambda x : A@x\n",
        "A_buggy = implicit2explicit_square_buggy( linearFcn, 4 )\n",
        "print(\"We think it is:\")\n",
        "matprint(A_buggy)\n",
        "print(\"It should be:\")\n",
        "matprint(A)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We think it is:\n",
            "      0        4        8       12  \n",
            "      1        5        9       13  \n",
            "      2        6       10       14  \n",
            "      3        7       11       15  \n",
            "It should be:\n",
            "      0        1        2        3  \n",
            "      4        5        6        7  \n",
            "      8        9       10       11  \n",
            "     12       13       14       15  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJOeMhDsJzxK"
      },
      "source": [
        "to fix this bug, we don't use `A[col]` and so on, we use `A[:,col]` to make sure we're getting a column, not a row:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CHKrI2OJ9h4",
        "outputId": "e42c8941-8264-4ad8-eebe-d1e6ff85a227"
      },
      "source": [
        "def implicit2explicit_square_fixed( linearFcn, n ):\n",
        "  I   = np.eye(n)\n",
        "  y   = linearFcn( I[0] )\n",
        "  A   = np.zeros( (n,n), dtype=y.dtype )\n",
        "  for col in range(0,n):\n",
        "    A[:,col] = linearFcn( I[:,col] )\n",
        "  return A\n",
        "\n",
        "A_bugFixed = implicit2explicit_square_fixed( linearFcn, 4 )\n",
        "print(\"Now it works:\")\n",
        "matprint(A_bugFixed)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now it works:\n",
            "      0        1        2        3  \n",
            "      4        5        6        7  \n",
            "      8        9       10       11  \n",
            "     12       13       14       15  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCwYPzV2KLH2"
      },
      "source": [
        "... so if you had this bug, you may have been swapping the transpose with the forward operator. This often shows up in your results by an incorrect **shift**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z08pZaW-pjQT"
      },
      "source": [
        "#### Now test our `implicit2explicit` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1p0DmKmbblS",
        "outputId": "31df304f-20f3-4bb6-8d8f-5937158d24f9"
      },
      "source": [
        "A   = np.arange(12).reshape(3,4) # 3 x 4 matrix\n",
        "def sampleLinearFunction(x):\n",
        "  return A@x\n",
        "\n",
        "def trickierLinearFunction(X):\n",
        "  \"\"\" 'Tricky' since a matrix domain \"\"\"\n",
        "  return np.trace(X)\n",
        "\n",
        "def veryTrickierLinearFunction(X):\n",
        "  \"\"\" 'Trickier' since complex valued \"\"\"\n",
        "  return 1j*np.trace(X)\n",
        "\n",
        "Amat = implicit2explicit( sampleLinearFunction, 4)\n",
        "Amat = implicit2explicit( sampleLinearFunction, (4,1) ) # also works\n",
        "print(\"The matrix we found is\")\n",
        "matprint(Amat)\n",
        "print(\"The original matrix was\")\n",
        "matprint(A)\n",
        "\n",
        "print(\"The trace of a 3x3 matrix, as a matrix operator:\")\n",
        "Amat = implicit2explicit( trickierLinearFunction, (3,3) )\n",
        "matprint(Amat)\n",
        "\n",
        "print(\"The trace of a 3x3 matrix, as a matrix operator, with imaginary numbers:\")\n",
        "Amat = implicit2explicit( veryTrickierLinearFunction, (3,3) )\n",
        "matprint(Amat)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The matrix we found is\n",
            "      0        1        2        3  \n",
            "      4        5        6        7  \n",
            "      8        9       10       11  \n",
            "The original matrix was\n",
            "      0        1        2        3  \n",
            "      4        5        6        7  \n",
            "      8        9       10       11  \n",
            "The trace of a 3x3 matrix, as a matrix operator:\n",
            "      1        0        0        0        1        0        0        0        1  \n",
            "The trace of a 3x3 matrix, as a matrix operator, with imaginary numbers:\n",
            "   0+1j     0+0j     0+0j     0+0j     0+1j     0+0j     0+0j     0+0j     0+1j  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKld6dzLtPt6"
      },
      "source": [
        "## Adjoint test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGyCD8i7zlgK"
      },
      "source": [
        "#### Let's make a different test of the adjoint\n",
        "This one will scale easily to huge values of $N$\n",
        "\n",
        "The adjoint of a linear operator $A$ is the operator (let's call it $T$, but later we'll denote it $A^*$ or $A^\\dagger$ or $A^H$ in physics) such that for any inputs $x$ and $y$ of the right size,\n",
        "$$ \\langle Ax, y \\rangle = \\langle x, Ty \\rangle. $$\n",
        "\n",
        "This inner product we use `np.vdot` for, as it does the right thing (adds in a complex conjugat, and automatically vectorizes it, so it's the right thing to do for a matrix).  `np.dot` does the right thing *only* if our input is 1D and real-valued.\n",
        "\n",
        "So, we'll just check this equation for several random choices of $x$ and $y$.  This is no way to **prove** that the adjoint is implemented correctly, but if we pass a lot of these checks, it's good evidence we've done it correctly.  If we fail a check, then we know for sure that there is a problem.\n",
        "\n",
        "Note that for a matrix, there's also the formula \n",
        "$$ A^* = \\overline{A}^T$$\n",
        "meaning the adjoint is the transpose of the complex conjugate. So for a real-valued matrix, the adjoint is just the **transpose**.\n",
        "\n",
        "Btw, do not confuse adjoint with [\"adjugate\"](https://en.wikipedia.org/wiki/Adjugate_matrix), which is a classical term used when finding the inverse of a matrix by hand "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYkyK32uyOgT",
        "outputId": "fd8d867c-a91a-4e83-da9e-6a1e70bc709d"
      },
      "source": [
        "def test_adjoint( A, At, domainSize, nRep = 10):\n",
        "  rng   = np.random.default_rng()\n",
        "  for rep in range(nRep):\n",
        "    x  = rng.normal( size=domainSize )\n",
        "    Ax = A(x)\n",
        "    y  = rng.normal( size=Ax.shape)\n",
        "    Aty= At(y)\n",
        "\n",
        "    er  = np.vdot(Ax,y) - np.vdot(x,Aty)\n",
        "    er  = np.abs(er)/np.sqrt( norm(x)*norm(y) ) # nice scaling\n",
        "    print(f\"Relative error in adjoint: {er:.2e}\")\n",
        "\n",
        "\n",
        "A   = np.arange(12).reshape(3,4) # test it on something rectangular (not square) to help expose more bugs\n",
        "\n",
        "linop     = lambda x : A@x\n",
        "linop_adj = lambda y : A.T@y\n",
        "\n",
        "test_adjoint( linop, linop_adj, A.shape[1] )"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Relative error in adjoint: 0.00e+00\n",
            "Relative error in adjoint: 3.58e-15\n",
            "Relative error in adjoint: 2.32e-15\n",
            "Relative error in adjoint: 2.03e-15\n",
            "Relative error in adjoint: 3.01e-16\n",
            "Relative error in adjoint: 6.83e-16\n",
            "Relative error in adjoint: 6.01e-16\n",
            "Relative error in adjoint: 3.33e-15\n",
            "Relative error in adjoint: 0.00e+00\n",
            "Relative error in adjoint: 2.37e-15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qZDd-U5vZDv"
      },
      "source": [
        "## Power method to estimate spectral norms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz_FK6tcXzYb",
        "outputId": "3e57c0da-ebe2-445c-d5ef-beff6560e73b"
      },
      "source": [
        "A   = np.arange(12).reshape(3,4) # test it on something rectangular (not square) to help expose more bugs\n",
        "\n",
        "normA = norm(A,ord=2)\n",
        "print(\"Spectral norm of A is\", normA )\n",
        "\n",
        "def powerMethod(A, At, domainSize=None, x=None, iters=100, tol=1e-6):\n",
        "  if x is None:\n",
        "    if domainSize is None:\n",
        "      raise ValueError(\"need domain size or x0 to be specified\")\n",
        "    rng   = np.random.default_rng()\n",
        "    x = rng.normal( size=domainSize )\n",
        "  normalization = norm( x.ravel() ) # Euclidean/Frobenius norm\n",
        "  for k in range(iters):\n",
        "    x = np.real_if_close( At(A(x)) )\n",
        "    oldNormalization = normalization\n",
        "    normalization = norm( x.ravel() )\n",
        "    x /= normalization  # do this in-place\n",
        "    if abs(oldNormalization - normalization)/np.max( [1e-12,normalization] ) < tol:\n",
        "      print(\"Reached tolerance after\",k,\"iterations.\")\n",
        "      break\n",
        "  return np.sqrt(normalization)\n",
        "\n",
        "normA_v2 = powerMethod(linop,linop_adj, A.shape[1], iters=1000, tol=1e-12)\n",
        "print(\"Spectral norm of A is\", normA_v2, \"using power method\" )\n",
        "print(\"  so the error in the power method is\", abs(normA-normA_v2) )"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spectral norm of A is 22.40929816327044\n",
            "Reached tolerance after 4 iterations.\n",
            "Spectral norm of A is 22.40929816327043 using power method\n",
            "  so the error in the power method is 7.105427357601002e-15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eqx4YeetvN_2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}